{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Data Preparation</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing tensorflow to load mnist database\n",
    "import tensorflow as tf\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Building the classifier</h3>\n",
    "<div>https://scikit-learn.org/stable/modules/neural_networks_supervised.html</div>\n",
    "<div>https://scikitlearn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier.fit</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.31342783\n",
      "Iteration 2, loss = 0.25560445\n",
      "Iteration 3, loss = 0.24057617\n",
      "Iteration 4, loss = 0.24962298\n",
      "Iteration 5, loss = 0.26456192\n",
      "Iteration 6, loss = 0.20037670\n",
      "Iteration 7, loss = 0.15379867\n",
      "Iteration 8, loss = 0.13941311\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "\n",
    "#flatten like in the HW\n",
    "x_train_flatten = x_train.reshape(x_train.shape[0],x_train.shape[1]*x_train.shape[2])\n",
    "x_test_flatten = x_test .reshape(x_test.shape[0],x_test.shape[1]*x_test.shape[2])\n",
    "\n",
    "#scale the inputs\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train_flatten)\n",
    "x_train_ready = scaler.transform(x_train_flatten)\n",
    "x_test_ready = scaler.transform(x_test_flatten)\n",
    "\n",
    "#Using stochastic gradient descent\n",
    "clf = MLPClassifier(solver='sgd', alpha=1e-5,hidden_layer_sizes=(50,),max_iter=8\n",
    "                    , verbose=True, random_state=1,\n",
    "                    learning_rate_init=.1)\n",
    "\n",
    "#https://scikit-learn.org/stable/auto_examples/neural_networks/plot_mnist_filters.html\n",
    "#Catching the warning to get rid of the error message\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=ConvergenceWarning,\n",
    "                            module=\"sklearn\")\n",
    "    clf.fit(x_train_ready,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.964217\n",
      "Test set score: 0.947500\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set score: %f\" % clf.score(x_train_ready, y_train))\n",
    "print(\"Test set score: %f\" % clf.score(x_test_ready, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I decided to stick with 2 hidden layers and max iterations of 8 for my network (the loss seemed to start fluctuating around there, but below I made a loop that optimizes the hidden layer sizes for the best output in intervals of 2 between 5 and 100. Might take a bit of time to run but should result in optimal layer size. Commenteded out to save resources when you run the entire project, but when I ran it it took around ~5min and got a value of 91."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting for , 5  ...\n",
      "Fitting for , 7  ...\n",
      "Fitting for , 9  ...\n",
      "Fitting for , 11  ...\n",
      "Fitting for , 13  ...\n",
      "Fitting for , 15  ...\n",
      "Fitting for , 17  ...\n",
      "Fitting for , 19  ...\n",
      "Fitting for , 21  ...\n",
      "Fitting for , 23  ...\n",
      "Fitting for , 25  ...\n",
      "Fitting for , 27  ...\n",
      "Fitting for , 29  ...\n",
      "Fitting for , 31  ...\n",
      "Fitting for , 33  ...\n",
      "Fitting for , 35  ...\n",
      "Fitting for , 37  ...\n",
      "Fitting for , 39  ...\n",
      "Fitting for , 41  ...\n",
      "Fitting for , 43  ...\n",
      "Fitting for , 45  ...\n",
      "Fitting for , 47  ...\n",
      "Fitting for , 49  ...\n",
      "Fitting for , 51  ...\n",
      "Fitting for , 53  ...\n",
      "Fitting for , 55  ...\n",
      "Fitting for , 57  ...\n",
      "Fitting for , 59  ...\n",
      "Fitting for , 61  ...\n",
      "Fitting for , 63  ...\n",
      "Fitting for , 65  ...\n",
      "Fitting for , 67  ...\n",
      "Fitting for , 69  ...\n",
      "Fitting for , 71  ...\n",
      "Fitting for , 73  ...\n",
      "Fitting for , 75  ...\n",
      "Fitting for , 77  ...\n",
      "Fitting for , 79  ...\n",
      "Fitting for , 81  ...\n",
      "Fitting for , 83  ...\n",
      "Fitting for , 85  ...\n",
      "Fitting for , 87  ...\n",
      "Fitting for , 89  ...\n",
      "Fitting for , 91  ...\n",
      "Fitting for , 93  ...\n",
      "Fitting for , 95  ...\n",
      "Fitting for , 97  ...\n",
      "Fitting for , 99  ...\n"
     ]
    }
   ],
   "source": [
    "def optimize():\n",
    "    s = 0 \n",
    "    maxScore = 0\n",
    "    #5<size<51, step sizes of 5\n",
    "    for i in range(5,101,2):\n",
    "        print(\"Fitting for \",i,\" ...\")\n",
    "        clf = MLPClassifier(solver='sgd', alpha=1e-5,hidden_layer_sizes=(i,),max_iter=8\n",
    "                 , verbose=False, random_state=1,\n",
    "                learning_rate_init=.1)\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\", category=ConvergenceWarning,\n",
    "                                    module=\"sklearn\")\n",
    "            clf.fit(x_train_ready,y_train)\n",
    "            score = clf.score(x_test_ready, y_test)\n",
    "        if(maxScore < score):\n",
    "            maxScore = score\n",
    "            s = i\n",
    "            \n",
    "    return s\n",
    "\n",
    "p = optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal values:  91\n",
      "Iteration 1, loss = 0.29100477\n",
      "Iteration 2, loss = 0.19626521\n",
      "Iteration 3, loss = 0.25345731\n",
      "Iteration 4, loss = 0.28562124\n",
      "Iteration 5, loss = 0.26906339\n",
      "Iteration 6, loss = 0.24219481\n",
      "Iteration 7, loss = 0.19200839\n",
      "Iteration 8, loss = 0.17674323\n",
      "Training set score: 0.979800\n",
      "Test set score: 0.962900\n"
     ]
    }
   ],
   "source": [
    " #Uncomment if you want to do the above calculation yourself (~5min)\n",
    "print(\"Optimal values: \",p)\n",
    "\n",
    "clf = MLPClassifier(solver='sgd', alpha=1e-5,hidden_layer_sizes=(p,),max_iter=8\n",
    "                    , verbose=True, random_state=1,\n",
    "                    learning_rate_init=.1)\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=ConvergenceWarning,\n",
    "                                        module=\"sklearn\")\n",
    "    clf.fit(x_train_ready,y_train)\n",
    "\n",
    "print(\"Training set score: %f\" % clf.score(x_train_ready, y_train))\n",
    "print(\"Test set score: %f\" % clf.score(x_test_ready, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, all the performance didn't change much when I modified other parameters besides layer size. Increasing the iterations past 8 didn't have a big effect either as the loss seemed to hover around a set value. Most of the 'good' models had a score around 0.95 to 0.98."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
